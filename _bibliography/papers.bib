---
---

@string{aps = {American Physical Society,}}


@article{10.1007/978-3-031-20713-6_14,
abbr={2DSensorFusion},
author={Pasandi, Morteza Mousa
and Liu, Tianran
and Massoud, Yahya
and Lagani{\`e}re, Robert},
editor={Bebis, George
and Li, Bo
and Yao, Angela
and Liu, Yang
and Duan, Ye
and Lau, Manfred
and Khadka, Rajiv
and Crisan, Ana
and Chang, Remco},
title={Sensor Fusion Operators for Multimodal 2D Object Detection},
booktitle={"Advances in Visual Computing"},
year={2022},
publisher={"Springer International Publishing"},
address={"Cham"},
pages={"184--195"},
abstract={"Autonomous driving requires effective capabilities to detect road objects in different environmental conditions. One promising solution to improve perception is to leverage multi-sensor fusion. This approach aims to combine various sensor streams in order to best integrate the information coming from the different sensors. Fusion operators are used to combine features from different modalities inside convolutional neural network architectures. In this study, we provide a framework for evaluating early fusion operators using different 2D object detection architectures. This comparative study includes element-wise addition and multiplication, feature concatenation, multi-modal factorized bilinear pooling, and bilaterally-guided fusion. We report quantitative results of the performance as well as an analysis of computational costs of these operators on different architectures."},
isbn={"978-3-031-20713-6"},
url={https://link.springer.com/chapter/10.1007/978-3-031-20713-6_14},
pdf={978-3-031-20713-6_14.pdf},
dimensions={true},
selected={false}
}

@article{Massoud2022,
abbr={3DSensorFusion},
author = "Yahya Massoud and Robert Laganiere",
title = "{Learnable Fusion Mechanisms for Object Detection in Autonomous Vehicles}",
year = "2022",
month = "11",
abstract={"In this work, we propose a novel deep learning based sensor fusion framework, that uses both camera and LiDAR sensors in a multi-modal and multi-view setting. In order to leverage both data streams, we incorporate two new sophisticated fusion mechanisms: element-wise multiplication and multi-modal factorized bilinear pooling. When compared to previously used fusion operators such as element-wise addition and concatenation of feature maps, our proposed fusion methods significantly increase the bird’s eye view moderate average precision score by +4.97% and +8.35% for both methods, respectively, when evaluated on KITTI dataset for object detection. Furthermore, we provide a detailed study of important design choices that contribute to the performance of deep learning based sensor fusion frameworks such as data augmentation, multi-task learning, and the design of the convolutional architecture. Finally, we provide qualitative results that showcase both success and failure cases for our proposed framework. We also discuss directions for mitigating failure cases. "},
url ={https://www.techrxiv.org/articles/preprint/Learnable_Fusion_Mechanisms_for_Object_Detection_in_Autonomous_Vehicles/21506124},
doi = "10.36227/techrxiv.21506124.v1",
pdf={Massoud_Laganiere-Learnable_Fusion_Mechanisms_for_Object_Detection_in_Autonomous_Vehicles (3).pdf},
selected={true}
}